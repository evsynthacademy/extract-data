---
  title: " Ideal vs Practical
 "
---


## Ideal vs Practical


Let’s start with how you train and monitor your extractors:
<br><br>
- **Ideal**: Train all of your data extractors in advance<br><br>
- **Practical**: Train your data extractors as you go<br><br>
Even if you carefully train your team far in advance, you’re going to find some inconsistencies in how each person fills out the data extraction form. You’re working with people after all, not machines. <br><br>
Take a look at this side by side comparison of how two research assistants extracted data from the same study:<br><br>

<center>
<img src="{{site.baseurl}}/img/sidebyside.jpg">
</center>


You’ll notice that reviewer A’s form includes just the basic information, while reviewer B’s form includes a lot of details, especially about depression incidence.
Reviewer A remembered to write “NR” or “Not Reported” for fields where the study didn’t have related information. Reviewer B just left those fields blank. 
<br><br>
The reviewers also disagree on a few fields: check out their answers for Health Related QoL and depression incidence. 
<br><br>
So which reviewer is better? Neither one. Both filed out the form to the best of their ability; they just interpreted what they were supposed to do differently. 
<br><br>
Data extraction is difficult. You want to make it as easy as possible for your reviewers to follow the same protocol.
<br><br>
That’s why it’s important to create an extraction form that’s as clear as possible about how to collect and organize your data. Use tools like drop down menus, specific choices, and instructions embedded in the form to help your extractors stay on track. This is where a program like SRDR can be very useful because it includes the ability to create these kind of tools to minimize data extraction variations. 
<br><br>
Also, be sure to frequently check in with each reviewer or have team meetings to ensure everyone is following a standardized approach. 

